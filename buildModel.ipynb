{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "velvet-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bearing-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entertaining-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pleasant-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./data/train\"\n",
    "test_dir = \"./data/test\"\n",
    "MODEL_NAME = './cancer.keras'\n",
    "LITE_MODEL = './cancer.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bulgarian-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class earlyCancel(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('loss')<0.3):\n",
    "            print(\"\\nLoss is low so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "earlycancel = earlyCancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "declared-valley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2506 images belonging to 2 classes.\n",
      "Found 131 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255,\n",
    "#     rotation_range=40, # range from 0 to 180 degrees; random\n",
    "#     width_shift_range=0.2, # shift object from center (as proportion of image size); how does it know object??\n",
    "#     height_shift_range=0.2, # shifts object from center(as proportion of image size); how does it know object?? \n",
    "#     shear_range=0.2, # by portion of image; 0.2 will shear up to 20% of image\n",
    "#     zoom_range=0.2, # relative portion too. \n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,\n",
    "#     fill_mode='nearest',\n",
    "    validation_split=0.05,\n",
    "    )\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224,224), # this is useful for loading in data that isn't this size; will resize as it's loaded in\n",
    "    batch_size=32, # whole science for selecting this; beyond scope of this class\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    "    )\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224,224), # this is useful for loading in data that isn't this size; will resize as it's loaded in\n",
    "    batch_size=128, # whole science for selecting this; beyond scope of this class\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-livestock",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dense-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(224, 224, 3, ))\n",
    "\n",
    "rscl = tf.keras.layers.experimental.preprocessing.Rescaling(scale=1./255)(inputs)\n",
    "\n",
    "c2d1 = tf.keras.layers.Conv2D(8, (3,3), activation='swish', padding='Same')(rscl)\n",
    "\n",
    "avp1 = tf.keras.layers.AveragePooling2D(2, 2)(c2d1)\n",
    "c2d2a = tf.keras.layers.Conv2D(16, (3,3), activation='swish', padding='Same')(avp1)\n",
    "avp2 = tf.keras.layers.AveragePooling2D(2, 2)(c2d2a)\n",
    "c2d3a = tf.keras.layers.Conv2D(32, (3,3), activation='swish', padding='Same')(avp2)\n",
    "avp3 = tf.keras.layers.AveragePooling2D(2, 2)(c2d3a)\n",
    "c2d4a = tf.keras.layers.Conv2D(64, (3,3), activation='swish', padding='Same')(avp3)\n",
    "avp4 = tf.keras.layers.AveragePooling2D(2, 2)(c2d4a)\n",
    "\n",
    "mxp1 = tf.keras.layers.MaxPool2D(2, 2)(c2d1)\n",
    "c2d2m = tf.keras.layers.Conv2D(16, (3,3), activation='swish', padding='Same')(mxp1)\n",
    "mxp2 = tf.keras.layers.MaxPool2D(2, 2)(c2d2m)\n",
    "c2d3m = tf.keras.layers.Conv2D(32, (3,3), activation='swish', padding='Same')(mxp2)\n",
    "mxp3 = tf.keras.layers.MaxPool2D(2, 2)(c2d3m)\n",
    "c2d4m = tf.keras.layers.Conv2D(64, (3,3), activation='swish', padding='Same')(mxp3)\n",
    "mxp4 = tf.keras.layers.MaxPool2D(2, 2)(c2d4m)\n",
    "\n",
    "ctc = tf.keras.layers.Concatenate()([avp4, mxp4])\n",
    "\n",
    "flt = tf.keras.layers.Flatten()(ctc)\n",
    "\n",
    "dns = tf.keras.layers.Dense(512, activation='swish')(flt)\n",
    "\n",
    "drp = tf.keras.layers.Dropout(0.2)(dns)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(drp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hindu-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "collective-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(amsgrad=True),\n",
    "    metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "declared-hobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "79/79 [==============================] - 12s 103ms/step - loss: 0.8607 - acc: 0.5851 - val_loss: 0.6711 - val_acc: 0.6260\n",
      "Epoch 2/25\n",
      "79/79 [==============================] - 7s 86ms/step - loss: 0.5497 - acc: 0.7003 - val_loss: 0.4862 - val_acc: 0.7176\n",
      "Epoch 3/25\n",
      "79/79 [==============================] - 7s 88ms/step - loss: 0.4251 - acc: 0.7959 - val_loss: 0.4843 - val_acc: 0.7481\n",
      "Epoch 4/25\n",
      "79/79 [==============================] - 7s 85ms/step - loss: 0.3870 - acc: 0.8201 - val_loss: 0.3533 - val_acc: 0.8244\n",
      "Epoch 5/25\n",
      "79/79 [==============================] - 7s 88ms/step - loss: 0.3670 - acc: 0.8167 - val_loss: 0.5359 - val_acc: 0.7634\n",
      "Epoch 6/25\n",
      "79/79 [==============================] - 7s 84ms/step - loss: 0.3929 - acc: 0.8139 - val_loss: 0.4866 - val_acc: 0.7405\n",
      "Epoch 7/25\n",
      "79/79 [==============================] - 7s 86ms/step - loss: 0.3614 - acc: 0.8193 - val_loss: 0.3456 - val_acc: 0.8321\n",
      "Epoch 8/25\n",
      "79/79 [==============================] - 7s 87ms/step - loss: 0.3327 - acc: 0.8489 - val_loss: 0.3633 - val_acc: 0.8168\n",
      "Epoch 9/25\n",
      "79/79 [==============================] - 7s 85ms/step - loss: 0.3348 - acc: 0.8415 - val_loss: 0.3476 - val_acc: 0.8168\n",
      "Epoch 10/25\n",
      "79/79 [==============================] - 7s 84ms/step - loss: 0.2979 - acc: 0.8521 - val_loss: 0.3952 - val_acc: 0.8321\n",
      "Epoch 11/25\n",
      "79/79 [==============================] - 7s 87ms/step - loss: 0.2975 - acc: 0.8641 - val_loss: 0.3727 - val_acc: 0.8244\n",
      "\n",
      "Loss is low so cancelling training!\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator, # generator that we set up earlier; streams images from directory\n",
    "    epochs=25, \n",
    "    validation_data=val_generator,\n",
    "    callbacks=[earlycancel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hearing-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-costume",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "different-paste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 244, 244, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "rescaling (Rescaling)           (None, 244, 244, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 244, 244, 8)  224         rescaling[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 122, 122, 8)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 122, 122, 8)  0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 122, 122, 16) 1168        average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 122, 122, 16) 1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 61, 61, 16)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 61, 61, 16)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 61, 61, 32)   4640        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 61, 61, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 30, 30, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 30, 30, 32)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 30, 30, 64)   18496       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 30, 30, 64)   18496       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 15, 15, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 15, 15, 64)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 15, 15, 128)  0           average_pooling2d_3[0][0]        \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 28800)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          14746112    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,795,457\n",
      "Trainable params: 14,795,457\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "annual-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "#     rescale=1./255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "successful-acoustic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 660 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(244,244),\n",
    "    batch_size=32,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "patient-bottle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 2s 83ms/step - loss: 0.3654 - acc: 0.8258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.36543479561805725, 0.8257575631141663]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-qualification",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-tribune",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-jewelry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "placed-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "static-restaurant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpsrrpnsf0/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "meaningful-albany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12913936"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(LITE_MODEL, \"wb\").write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-margin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-oxide",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "coastal-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=LITE_MODEL)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "refined-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "modified-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "uploaded_file = open('./data/train/benign/3.jpg', 'rb')\n",
    "uploaded_file = open('./data/train/malignant/5.jpg', 'rb')\n",
    "img = Image.open(uploaded_file)\n",
    "img = img.resize((224,224))\n",
    "img = np.array(img).astype(np.float32)\n",
    "img = np.array([img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "official-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on random input data.\n",
    "input_data = img\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "naked-acceptance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.590647]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-roads",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
